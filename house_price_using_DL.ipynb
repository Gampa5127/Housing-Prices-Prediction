{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom tensorflow import keras\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras import layers","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"# load the data\n\n# reading the train data as X\nX = pd.read_csv('../input/home-data-for-ml-course/train.csv', index_col='Id')\n\n# reading the test data as X_test\nX_test = pd.read_csv('../input/home-data-for-ml-course/test.csv', index_col='Id')\n\n# Examine the data\nprint(X.shape)\nprint(X_test.shape)\nprint(X.columns)\nprint(X_test.columns)\nprint(X.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# finding and removing the data which don't have the target values\n\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\n# seperating the target values\ny = X.pop('SalePrice')\n\nprint(X.shape)\nprint(y.head())","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# printing categorical column labels with cardinality\n\nfor i in X.columns:\n    if X[i].dtype == 'object':\n        print(i,X[i].nunique(), sep='-')\n\n# removing the columns from X and X_test which have cardinality more than 15\n\nlist = []\nfor i in X.columns:\n    if X[i].dtype == 'object' and X[i].nunique() > 15:\n        list.append(i)\n\nprint(len(list))\nX.drop(list, axis=1, inplace=True)  \nX_test.drop(list, axis=1, inplace=True)\nprint(X.shape, X_test.shape)\n\n#removing the columns which have more than 1000 null values \nmissing_columns =[]\nfor i in X.columns:\n    if X[i].isnull().sum()>650:\n        missing_columns.append(i)\nprint(missing_columns)\nX.drop(missing_columns, axis=1, inplace = True)\nX_test.drop(missing_columns, axis = 1, inplace = True)\nprint(X.shape, X_test.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dividing the data columns into numerical and catergorical columns\n\nnumerical_columns = []\ncategorical_columns = []\nfor i in X.columns:\n    if X[i].dtype == 'object':\n        categorical_columns.append(i)\n    elif X[i].dtype in ['int64', 'float64']:\n        numerical_columns.append(i)\nprint(len(numerical_columns))\nprint(len(categorical_columns))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# imputing numerical_columns\n\n# for train data\nfor i in numerical_columns:\n    current_column = np.array(X[i]).reshape(-1,1)\n    updated_column = SimpleImputer().fit_transform(current_column)\n    X[i] = updated_column\n\n# for test data\nfor i in numerical_columns:\n    current_column = np.array(X_test[i]).reshape(-1,1)\n    updated_column = SimpleImputer().fit_transform(current_column)\n    X_test[i] = updated_column\n    \n# imputing catergorical_columns\n\n# for train data\nfor i in categorical_columns:\n    current_column = np.array(X[i]).reshape(-1,1)\n    updated_column = SimpleImputer(strategy = 'most_frequent').fit_transform(current_column)\n    X[i] = updated_column\n    \n# for test data\nfor i in categorical_columns:\n    current_column = np.array(X_test[i]).reshape(-1,1)\n    updated_column = SimpleImputer(strategy = 'most_frequent').fit_transform(current_column)\n    X_test[i] = updated_column\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# removing the outliers in numerical columns\nnumerical_columns = np.array(numerical_columns).reshape(12,3)\nfig, ax = plt.subplots(12,3, figsize = (30,50))\nfor i in range(12):\n    for j in range(3):\n        sns.boxplot(y=X[numerical_columns[i][j]], data = X, ax = ax[i,j] )\n\nX = pd.concat([X,y], axis=1)\nX.drop(X[X['LotFrontage']>250].index, inplace=True)\n\nX.drop(X[X['BsmtFinSF1']>4000].index, inplace=True)\n\nX.drop(X[X['LotArea']>100000].index, inplace=True)\n\nX.drop(X[X['TotalBsmtSF']>4000].index, inplace=True)\n\nX.drop(X[X['GrLivArea']>4000].index, inplace=True)\n\n\n# seperating the target values\ny = X.pop('SalePrice')\nprint(X.shape, X_test.shape, y.shape)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# encoding categorical columns\n\nohec = OneHotEncoder(handle_unknown='ignore', sparse=False)\n# fitting and transforming categorical train and test data using ohc and changing the column names\nX_cat = pd.DataFrame(ohec.fit_transform(X[categorical_columns]))\nX_cat.columns = ohec.get_feature_names(categorical_columns)\nXtest_cat = pd.DataFrame(ohec.transform(X_test[categorical_columns]))\nXtest_cat.columns = ohec.get_feature_names(categorical_columns)\n# giving the index of train data to categorical train data\nX_cat.index = X.index\nXtest_cat.index = X_test.index\n# dropping the catergorical columns from original data\nX.drop(categorical_columns,axis=1,inplace = True)\nX_test.drop(categorical_columns, axis=1, inplace = True)\n# concating the catergorical data with original data\nX = pd.concat([X,X_cat], axis=1)\nX_test = pd.concat([X_test, Xtest_cat], axis= 1)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# splitting the training data into training data(75%) and validation data(25%)\nX_train, X_valid, y_train, y_valid = train_test_split(X,y, train_size = 0.95)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"markdown","source":"#train and fitting the random forest\nmodel = RandomForestRegressor()\nmodel.fit(X_train, y_train)\npred_valid = model.predict(X_valid)\nmae = mean_absolute_error(y_valid, pred_valid)\nprint('MAE: ', mae )\n   "},{"metadata":{"trusted":true},"cell_type":"code","source":"input_ = [X_train.shape[0]]\nmodel = keras.Sequential([\n    layers.BatchNormalization(),\n    layers.Dense(units=64,activation = 'relu', input_shape=input_),\n    \n    layers.Dense(units=32,activation = 'relu'),\n   \n    layers.Dense(units=16,activation = 'relu'),\n    \n    layers.Dense(units=8,activation = 'relu'),\n\n    layers.Dense(units=1)\n])\nmodel.compile(optimizer='adam', loss='mae')\nearlystopping = EarlyStopping(min_delta= 0.001, patience = 20, restore_best_weights = True)\nhistory = model.fit(X_train, y_train, validation_data = (X_valid,y_valid), batch_size = 128, epochs=1000, \n                    callbacks=[earlystopping], verbose=0)\npd.DataFrame(history.history).plot()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"pred_test = model.predict(X_test)\nsubmission = pd.DataFrame({'ID':X_test.index, 'SalePrice':pred_test.flatten()})\nprint(submission.head())\nsubmission.to_csv('submission.csv',index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}